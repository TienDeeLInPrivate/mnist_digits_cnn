{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying a CNN to classify handwritten digits in the MNIST Digits dataset\n",
    "\n",
    "Quickstart guide:\n",
    "\n",
    "1. Create a new virtual environment:\n",
    "    ```bash\n",
    "    python3 -m venv venv\n",
    "    ```\n",
    "2. Activate the virtual environment (macOS):\n",
    "    ```bash\n",
    "    source venv/bin/activate\n",
    "    ```\n",
    "3. Install required packages:\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "4. Create a [wandb](https://wandb.ai) account (if none exists) and log into wandb. For detailed documentation, visit the [official wandb documentation](https://docs.wandb.ai).\n",
    "\n",
    "Ready to start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Data preprocessing involves the following steps:\n",
    "1. Load data from idx-ubyte files into numpy arrays.\n",
    "2. Check the balance of the data.\n",
    "3. Visualize five digits from the training dataset.\n",
    "4. Normalize and standardize all pixel values.\n",
    "5. Concatenate and re-split all data into 60% train, 20% validation and 20% test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an idx-ubyte file and return data as a numpy array\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "training_images_filepath = 'mnist_digits_data/train-images.idx3-ubyte'\n",
    "training_labels_filepath = 'mnist_digits_data/train-labels.idx1-ubyte'\n",
    "test_images_filepath = 'mnist_digits_data/t10k-images.idx3-ubyte'\n",
    "test_labels_filepath = 'mnist_digits_data/t10k-labels.idx1-ubyte'\n",
    "\n",
    "# Loading and reading the data\n",
    "train_images = read_idx(training_images_filepath)\n",
    "train_labels = read_idx(training_labels_filepath)\n",
    "test_images = read_idx(test_images_filepath)\n",
    "test_labels = read_idx(test_labels_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create and print talble showing label distribution\n",
    "Optional: Uncomment code below to show different data distribution for training, test and all labels\n",
    "\"\"\"\n",
    "all_labels = train_labels\n",
    "# all_labels = test_labels\n",
    "# all_labels = np.concatenate((train_labels, test_labels))\n",
    "\n",
    "labels, counts = np.unique(all_labels, return_counts=True)\n",
    "percentages = counts / counts.sum() * 100\n",
    "df = pd.DataFrame({'Digit Label': labels, 'Frequency': counts, 'Percentage': percentages})\n",
    "print(df.to_string(index=False, float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize first 5 images of the training set\n",
    "\"\"\"\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 2))  \n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(train_images[i], cmap='gray')  \n",
    "    ax.set_title(f'Label: {train_labels[i]}')  \n",
    "    ax.axis('off')  \n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate mean and standard deviation of the normalized pixel values\n",
    "\"\"\"\n",
    "images = np.concatenate((train_images, test_images), axis=0) \n",
    "\n",
    "# Calculate mean and standard deviation on normalized pixel values\n",
    "images = images.astype(np.float32) / 255\n",
    "mean = np.mean(images)\n",
    "std = np.std(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform normalization and standarization of all pixel values\n",
    "\"\"\"\n",
    "# Normalize pixel values to [0, 1] \n",
    "train_images = torch.tensor(train_images).unsqueeze(1).float() / 255\n",
    "test_images = torch.tensor(test_images).unsqueeze(1).float() / 255\n",
    "\n",
    "# Without standardization, the sweep takes longer and the accuracy is lower\n",
    "train_images = (train_images - mean) / std\n",
    "test_images = (test_images - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create TensorDataset \n",
    "Concatenate and re-split all data into 60% train, 20% validation and 20% test sets\n",
    "\"\"\"\n",
    "train_dataset = TensorDataset(train_images, torch.tensor(train_labels).long())\n",
    "test_dataset = TensorDataset(test_images, torch.tensor(test_labels).long())\n",
    "full_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model architecture and define hyperparameters\n",
    "\n",
    "This section implements the following steps:\n",
    "1. Initialize the `sweep_config` for the Weights & Biases sweep with selected hyperparameter combinations.\n",
    "2. Construct the CNN architecture.\n",
    "3. Initialize the network and global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create TensorDataset \n",
    "Concatenate and re-split all data into 60% train, 20% validation and 20% test sets\n",
    "\"\"\"\n",
    "# Set fixed number of epochs due to computational limitations\n",
    "n_epochs = 5\n",
    "\n",
    "# Configuration of the wandb sweep\n",
    "sweep_config = {\n",
    "    'method': 'grid',  # Search method: Grid search\n",
    "    'metric': {\n",
    "      'name': 'accuracy',  \n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.0001, 0.001, 0.01] # Possible values for learning rate\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64, 128] # Possible values for batch size\n",
    "        }\n",
    "    },\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"mnist-digits-cnn\") # Initialize the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define CNN architecture in Pytorch\n",
    "\"\"\"\n",
    "class simple_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # Default padding = 0, stride = 1\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d() # Default dropout value = 50% (0.5)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) \n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training) # Dropout is applied to output of first fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x) # Softmax to create probability distribution, required for nll_loss (not required for CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialise network according to previously defined architecture and global variables\n",
    "\"\"\"\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False # No GPU available\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "global best_accuracy\n",
    "global best_batch_size\n",
    "global best_learning_rate\n",
    "\n",
    "best_accuracy = 0.0  # Initialize with minimum starting value\n",
    "best_batch_size = None  \n",
    "best_learning_rate = None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and execute train and validation cycle \n",
    "\n",
    "This section implements the following steps:\n",
    "1. Define train and validation functions\n",
    "2. Create train_and_validate wrapper for wandb run\n",
    "3. Execute wandb run with wandb agent to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the training function with logging of the training loss in wandb\n",
    "\"\"\"\n",
    "def train(network, optimizer, loader):\n",
    "    network.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Log the training loss every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            avg_train_loss = train_loss / 100\n",
    "            wandb.log({\"train_loss\": avg_train_loss})\n",
    "            train_loss = 0  # Reset the train loss for the next 100 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the validation function with logging of the validation loss and accuracy in wandb\n",
    "\"\"\"\n",
    "def validation(epoch, network, loader):\n",
    "    network.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    total_batches = 0\n",
    "    total_samples = 0  \n",
    "\n",
    "    # Training the model\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = network(data)\n",
    "            validation_loss += F.nll_loss(output, target, reduction='mean').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            total_batches += 1  \n",
    "            total_samples += target.size(0)\n",
    "\n",
    "    # Calculate average validation loss across all batches\n",
    "    validation_loss /= total_batches\n",
    "\n",
    "    # Calculate accuracy percentage\n",
    "    accuracy = 100. * correct / total_samples\n",
    "    accuracy = float(accuracy)  \n",
    "\n",
    "    # Print for monitoring\n",
    "    print(\"Validation Loss: \", validation_loss)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "    # Logging in wandb\n",
    "    wandb.log({\"validation_loss\": validation_loss, \"epoch\": epoch, \"accuracy\": accuracy})\n",
    "\n",
    "    # Save the current best model to best_model.pth for final evaluation on test set\n",
    "    global best_accuracy, best_batch_size, best_learning_rate\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(network.state_dict(), 'best_model.pth')  # Save the best model based on accuracy\n",
    "        best_batch_size = wandb.config.batch_size  # Save current batch size from the sweep config\n",
    "        best_learning_rate = wandb.config.learning_rate  # Save current learning rate from the sweep config\n",
    "        print(f\"Saved new best model with accuracy {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define train_and_validate wrapper for wandb run \n",
    "\"\"\"\n",
    "def train_and_validate():\n",
    "    # Initialize a new W&B run\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "\n",
    "        # Setup model and optimizer\n",
    "        network = simple_cnn()  \n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        # Prepare DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=int(config.batch_size), shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)  \n",
    "        validation(0, network, val_loader)\n",
    "\n",
    "        # Training loop (5 Epochs)\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            print('Epoch:', epoch)\n",
    "            train(network, optimizer, train_loader)\n",
    "            validation(epoch, network, val_loader)\n",
    "\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execute run to perform the sweep\n",
    "\"\"\"\n",
    "wandb.agent(sweep_id, train_and_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final testing and evaluation\n",
    "\n",
    "This section implements the following steps:\n",
    "1. Evaluate best model on test set\n",
    "2. Compare best model agains dummy classifier\n",
    "3. Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create DataLoader for test data, evaluate the best model and print the final accuracy\n",
    "\"\"\"\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "# Create DataLoader for test data\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1000,\n",
    "    shuffle=False \n",
    ")\n",
    "\n",
    "def test():\n",
    "    network = simple_cnn()\n",
    "    network.load_state_dict(torch.load('best_model.pth'))\n",
    "    network.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability (argmax) \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            all_preds.extend(pred.squeeze().tolist())\n",
    "            all_targets.extend(target.tolist())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')\n",
    "\n",
    "test()\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement DummyClassifier from sklearn and evaluate on test set \n",
    "\"\"\"\n",
    "# Implement DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit([[0]] * len(all_targets), all_targets)  # dummy_clf.fit to get the most frequent class\n",
    "dummy_preds = dummy_clf.predict([[0]] * len(all_targets))  # Predict the most frequent class\n",
    "\n",
    "# Calculate and print the accuracy of the dummy classifier\n",
    "dummy_accuracy = accuracy_score(all_targets, dummy_preds)\n",
    "print(f'Dummy classifier accuracy: {dummy_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optional: print the previously saved optimal hyperparameters\n",
    "\"\"\"\n",
    "print(best_batch_size)\n",
    "print(best_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot and show confusion matrix\n",
    "\"\"\"\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "fig = px.imshow(cm,\n",
    "                labels=dict(x=\"Predicted Labels\", y=\"True Labels\", color=\"Count\"),\n",
    "                x=list(range(10)),\n",
    "                y=list(range(10)),\n",
    "                color_continuous_scale='Blues', \n",
    "                text_auto=True)  \n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Confusion Matrix',  \n",
    "    title_x=0.5,  \n",
    "    width=620,  \n",
    "    height=600,  \n",
    "    xaxis=dict(\n",
    "        tickmode='linear', tick0=0, dtick=1,\n",
    "        showgrid=True,  \n",
    "        tickfont=dict(size=15),  \n",
    "        title_font=dict(size=16)  \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickmode='linear', tick0=0, dtick=1,\n",
    "        showgrid=True,  \n",
    "        tickfont=dict(size=15),  \n",
    "        title_font=dict(size=16)  \n",
    "    ),\n",
    "    title_font=dict(size=24),  \n",
    "    title_pad=dict(b=10),  \n",
    "    margin=dict(l=10, r=10, t=40, b=10)  \n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    showline=True, linewidth=2, linecolor='black'\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    showline=True, linewidth=2, linecolor='black'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
